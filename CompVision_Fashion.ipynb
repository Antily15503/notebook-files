{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1510218e-d059-498d-9ac3-710a39e0a8da",
   "metadata": {},
   "source": [
    "## Machine Learning Notebook for Compter Vision using PyTorch\n",
    "`torchvision`: - base\n",
    "`torchvision.datasets` - get datasets and data loading functions for computer vision\n",
    "`torchvision.models` - get pretrained computer vision models that you can leverage for your own problems\n",
    "`torchvision.transofmrs` - functions for manipulating your vision data (images) to be suited for ML models\n",
    "`torch.utils.data.Dataset` - base dataset class for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981681d2-f9c2-41ab-9997-45fa8029cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import Torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1f68f",
   "metadata": {},
   "source": [
    "### FOR A FUTURE MODULE BUT functionizing training and evalulation/testing loops for further models\n",
    "training loop - `train_step()`\n",
    "\n",
    "testing loop - `test_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bf996256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Performs a training with model trying to learn on data_loader\"\"\"\n",
    "train_loss, train_acc = 0, 0\n",
    "model.train() # put model into training mode\n",
    "for batch, (image, label) in enumerate(train_dataloader): # for each loop that loops through the batch. enumerate lets us keep track of # of batches\n",
    "    image, label = image.to(device), label.to(device)\n",
    "\n",
    "    #Forward pass\n",
    "    label_pred = model(image)\n",
    "\n",
    "    #Calculate the loss\n",
    "    loss = loss_fn(label_pred, label)\n",
    "    train_loss += loss # accumulate the train loss\n",
    "    train_acc += accuracy(y_true = label,\n",
    "                          y_pred = label_pred)\n",
    "\n",
    "    #Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Loss Backward\n",
    "    loss.backward()\n",
    "\n",
    "    #Optimizer step NOTE: OPTIMIZER WOULD OPTIMIZE AFTER EVERY BATCH (since it is in the batch loop), RATHER THAN AFTER VIEWING ALL DATA\n",
    "    optimizer.step()\n",
    "\n",
    "# Divide totals by length of dataloader to get avg values\n",
    "train_loss /= len(data_loader)\n",
    "train_acc /= len(data_loader)\n",
    "print(f\"Train loss: {train_loss:.5} | Train acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46030c2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "FashionMNIST database: a database of pictures of clothes that's done in grayscale\n",
    "Download is included by PyTorch (in datasets list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9740fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where does the downloaded data go to?\n",
    "    train=True, # do we want the training dataset or testing dataset \n",
    "    download=True, #do we want to download\n",
    "    transform=torchvision.transforms.ToTensor(), # how do we want to transofrm the data? ()\n",
    "    target_transform=None # do we need to transform the labels (output)?\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # this is the testing dataset, so no train\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7648bd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes # command to see information of the different classes of our train_data\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c83bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = train_data.class_to_idx # same command as above but returns as a dictionary (class category paired with index)\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c8d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a DataLoader\n",
    "# A DataLoader turns a dataset (in this case images) into a Python iterable (batches / mini-batches of data sections)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set up a Batch Size HYPERPARAMATER\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef98de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 9, label size: <built-in method size of Tensor object at 0x000001DE960A19A0>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOWING SAMPLES OF OUR MODEL'S DATA\n",
    "randomIndex = torch.randint(0, len(train_features_batch), size=[1]).item() # visualize one random sample from our batch (BATCH_SIZE)\n",
    "img, label = train_features_batch[randomIndex], train_labels_batch[randomIndex] # fetch the \"image\" and label from our batch at randomIndex\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\") # displays the image (in grayscale)\n",
    "plt.title(class_names[label]) # sets title to label string (since label is a tensor)\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 0: Linear MNIST Model that's trained on the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d234f",
   "metadata": {},
   "source": [
    "#### Building a baseline comp. vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05316f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a flatten layer\n",
    "flatten_model = nn.Flatten() # flattens the image tensor of our model from [1, 28, 28] to [1, 784]\n",
    "\n",
    "# building the baseline model\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "# building the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8af2a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up an instance of our model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape = 784, # this is 28x28\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names) # one output node for every class\n",
    ").to(\"cpu\")\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081a643",
   "metadata": {},
   "source": [
    "#### Create a loss function and optimizer\n",
    "since we're working with multi-class data, loss function is `nn.CrossEntropyLoss()`\n",
    "\n",
    "optimizer will still be `torch.optim.SGD()`\n",
    "\n",
    "since we're working on a classification problem, we can use accuracy as an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40f2f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    return (correct/len(y_pred)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc5a70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup our loss and optimizer function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5aa45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block that checks how fast the model runs\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float,\n",
    "                     end: float,\n",
    "                     device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd885a",
   "metadata": {},
   "source": [
    "#### Creating a Training Loop and training the model (model 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c63fe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Viewed 0/60000 samples.\n",
      "Viewed 12800/60000 samples.\n",
      "Viewed 25600/60000 samples.\n",
      "Viewed 38400/60000 samples.\n",
      "Viewed 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:15,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.4764 | Test loss: 0.4825, Test acc: 82.9972\n",
      "Epoch: 1\n",
      "---------\n",
      "Viewed 0/60000 samples.\n",
      "Viewed 12800/60000 samples.\n",
      "Viewed 25600/60000 samples.\n",
      "Viewed 38400/60000 samples.\n",
      "Viewed 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:15<00:07,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.4556 | Test loss: 0.4725, Test acc: 83.6262\n",
      "Epoch: 2\n",
      "---------\n",
      "Viewed 0/60000 samples.\n",
      "Viewed 12800/60000 samples.\n",
      "Viewed 25600/60000 samples.\n",
      "Viewed 38400/60000 samples.\n",
      "Viewed 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:22<00:00,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.4436 | Test loss: 0.4714, Test acc: 83.5763\n",
      "Train time on cpu: 22.972 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# importing tkdm to make a simple and quick progress bar (check the github for more info)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set seed and start timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_CPU = timer()\n",
    "\n",
    "# setting the number of epochs (we'll keep this small for faster training time)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    # Training loop\n",
    "    train_loss = 0 # <- this is the TOTAL LOSS of ONE BATCH. will be devided to find the average loss per item\n",
    "    # second loop to loop through training batches\n",
    "    for batch, (image, label) in enumerate(train_dataloader): # for each loop that loops through the batch. enumerate lets us keep track of # of batches\n",
    "        model_0.train()\n",
    "        #Forward pass\n",
    "        label_pred = model_0(image)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = loss_fn(label_pred, label)\n",
    "        train_loss += loss # accumulate the train loss\n",
    "\n",
    "        #Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Loss Backward\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimizer step NOTE: OPTIMIZER WOULD OPTIMIZE AFTER EVERY BATCH (since it is in the batch loop), RATHER THAN AFTER VIEWING ALL DATA\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printing out values\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Viewed {batch * len(image)}/{len(train_dataloader.dataset)} samples.\")\n",
    "\n",
    "        # END OF BATCH LOOP\n",
    "    \n",
    "    #Divide total trian loss by length of train dataloader\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    #Testing\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for image_test, label_test in test_dataloader:\n",
    "            #Forward pass\n",
    "            test_pred = model_0(image_test)\n",
    "\n",
    "            #Calculate the loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, label_test)\n",
    "\n",
    "            #Calculate accuracy\n",
    "            test_acc += accuracy(y_true=label_test, y_pred=test_pred.argmax(dim=1)) # converting test_pred from logits to prediction labels (argmax)\n",
    "\n",
    "        #Calculate the test loss and accuracy (avg value per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "        #END OF EVAL LOOP\n",
    "    \n",
    "    #Print out what's happening\n",
    "    print(f\"\\nTraining loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
    "\n",
    "#Calculate the training time\n",
    "train_time_end_CPU = timer()\n",
    "\n",
    "total_time_CPU = print_train_time(start=train_time_start_CPU,\n",
    "                                  end=train_time_end_CPU,\n",
    "                                  device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb8d0a",
   "metadata": {},
   "source": [
    "#### Making predictions and get results of Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88626d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module,    # defining a custom function and defining parameters to be passed in\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary contraining the results of mdoel predicting on data_loader.\"\"\"\n",
    "    loss, acc = 0 , 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Batch loop\n",
    "        for image, label in data_loader:\n",
    "            # make prediction\n",
    "            label_pred = model(image)\n",
    "\n",
    "            # Accumulate the loss and accuracy val per batch\n",
    "            loss += loss_fn(label_pred, label)\n",
    "            acc += accuracy_fn(y_true=label,\n",
    "                               y_pred=label_pred.argmax(dim=1))\n",
    "            # END BATCH LOOP\n",
    "        # find average loss and acc per batch (len(data_loader) = # of items per batch)\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    return {\"model_name\": model.__class__.__name__, # only works if models are created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ecb240b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47143012285232544,\n",
       " 'model_acc': 83.57627795527156}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model results on test data\n",
    "\n",
    "model_0_results = eval_model(model = model_0,\n",
    "                             data_loader = test_dataloader,\n",
    "                             loss_fn = loss_fn,\n",
    "                             accuracy_fn = accuracy)\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55424d7d",
   "metadata": {},
   "source": [
    "## MODEL 1: NONlinear MNIST Model that's trained on the GPU\n",
    "this model also borrows some things from Model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2c6d311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed479049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "501fc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with non-linear & linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units,),\n",
    "            nn.ReLU(),  # NON LINEAR LAYER TYPE (relu)\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6dabcde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of model_1\n",
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape = 784,\n",
    "                              hidden_units=10,\n",
    "                              output_shape = len(class_names)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8853f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a loss and optimizer function.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6671a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionizing training and evalulation loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
