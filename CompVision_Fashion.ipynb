{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1510218e-d059-498d-9ac3-710a39e0a8da",
   "metadata": {},
   "source": [
    "## Machine Learning Notebook for Compter Vision using PyTorch\n",
    "`torchvision`: - base\n",
    "`torchvision.datasets` - get datasets and data loading functions for computer vision\n",
    "`torchvision.models` - get pretrained computer vision models that you can leverage for your own problems\n",
    "`torchvision.transofmrs` - functions for manipulating your vision data (images) to be suited for ML models\n",
    "`torch.utils.data.Dataset` - base dataset class for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981681d2-f9c2-41ab-9997-45fa8029cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import Torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46030c2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "FashionMNIST database: a database of pictures of clothes that's done in grayscale\n",
    "Download is included by PyTorch (in datasets list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9740fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where does the downloaded data go to?\n",
    "    train=True, # do we want the training dataset or testing dataset \n",
    "    download=True, #do we want to download\n",
    "    transform=torchvision.transforms.ToTensor(), # how do we want to transofrm the data? ()\n",
    "    target_transform=None # do we need to transform the labels (output)?\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # this is the testing dataset, so no train\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7648bd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes # command to see information of the different classes of our train_data\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c83bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = train_data.class_to_idx # same command as above but returns as a dictionary (class category paired with index)\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c8d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a DataLoader\n",
    "# A DataLoader turns a dataset (in this case images) into a Python iterable (batches / mini-batches of data sections)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set up a Batch Size HYPERPARAMATER\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef98de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 4, label size: <built-in method size of Tensor object at 0x000001E34DE06EE0>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOWING SAMPLES OF OUR MODEL'S DATA\n",
    "randomIndex = torch.randint(0, len(train_features_batch), size=[1]).item() # visualize one random sample from our batch (BATCH_SIZE)\n",
    "img, label = train_features_batch[randomIndex], train_labels_batch[randomIndex] # fetch the \"image\" and label from our batch at randomIndex\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\") # displays the image (in grayscale)\n",
    "plt.title(class_names[label]) # sets title to label string (since label is a tensor)\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 0: Linear MNIST Model that's trained on the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d234f",
   "metadata": {},
   "source": [
    "#### Building a baseline comp. vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05316f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a flatten layer\n",
    "flatten_model = nn.Flatten() # flattens the image tensor of our model from [1, 28, 28] to [1, 784]\n",
    "\n",
    "# building the baseline model\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "# building the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8af2a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up an instance of our model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape = 784, # this is 28x28\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names) # one output node for every class\n",
    ").to(\"cpu\")\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081a643",
   "metadata": {},
   "source": [
    "#### Create a loss function and optimizer\n",
    "since we're working with multi-class data, loss function is `nn.CrossEntropyLoss()`\n",
    "\n",
    "optimizer will still be `torch.optim.SGD()`\n",
    "\n",
    "since we're working on a classification problem, we can use accuracy as an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40f2f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    return (correct/len(y_pred)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc5a70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup our loss and optimizer function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5aa45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block that checks how fast the model runs\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float,\n",
    "                     end: float,\n",
    "                     device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd885a",
   "metadata": {},
   "source": [
    "#### Creating a Training Loop and training the model (model 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c63fe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Viewed 0/60000 samples.\n",
      "Viewed 12800/60000 samples.\n",
      "Viewed 25600/60000 samples.\n",
      "Viewed 38400/60000 samples.\n",
      "Viewed 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:15,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.4764 | Test loss: 0.4825, Test acc: 82.9972\n",
      "Epoch: 1\n",
      "---------\n",
      "Viewed 0/60000 samples.\n",
      "Viewed 12800/60000 samples.\n",
      "Viewed 25600/60000 samples.\n",
      "Viewed 38400/60000 samples.\n",
      "Viewed 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:15<00:07,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.4556 | Test loss: 0.4725, Test acc: 83.6262\n",
      "Epoch: 2\n",
      "---------\n",
      "Viewed 0/60000 samples.\n",
      "Viewed 12800/60000 samples.\n",
      "Viewed 25600/60000 samples.\n",
      "Viewed 38400/60000 samples.\n",
      "Viewed 51200/60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:22<00:00,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.4436 | Test loss: 0.4714, Test acc: 83.5763\n",
      "Train time on cpu: 22.972 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# importing tkdm to make a simple and quick progress bar (check the github for more info)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set seed and start timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_CPU = timer()\n",
    "\n",
    "# setting the number of epochs (we'll keep this small for faster training time)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    # Training loop\n",
    "    train_loss = 0 # <- this is the TOTAL LOSS of ONE BATCH. will be devided to find the average loss per item\n",
    "    # second loop to loop through training batches\n",
    "    for batch, (image, label) in enumerate(train_dataloader): # for each loop that loops through the batch. enumerate lets us keep track of # of batches\n",
    "        model_0.train()\n",
    "        #Forward pass\n",
    "        label_pred = model_0(image)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = loss_fn(label_pred, label)\n",
    "        train_loss += loss # accumulate the train loss\n",
    "\n",
    "        #Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Loss Backward\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimizer step NOTE: OPTIMIZER WOULD OPTIMIZE AFTER EVERY BATCH (since it is in the batch loop), RATHER THAN AFTER VIEWING ALL DATA\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printing out values\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Viewed {batch * len(image)}/{len(train_dataloader.dataset)} samples.\")\n",
    "\n",
    "        # END OF BATCH LOOP\n",
    "    \n",
    "    #Divide total trian loss by length of train dataloader\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    #Testing\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for image_test, label_test in test_dataloader:\n",
    "            #Forward pass\n",
    "            test_pred = model_0(image_test)\n",
    "\n",
    "            #Calculate the loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, label_test)\n",
    "\n",
    "            #Calculate accuracy\n",
    "            test_acc += accuracy(y_true=label_test, y_pred=test_pred.argmax(dim=1)) # converting test_pred from logits to prediction labels (argmax)\n",
    "\n",
    "        #Calculate the test loss and accuracy (avg value per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "        #END OF EVAL LOOP\n",
    "    \n",
    "    #Print out what's happening\n",
    "    print(f\"\\nTraining loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
    "\n",
    "#Calculate the training time\n",
    "train_time_end_CPU = timer()\n",
    "\n",
    "total_time_CPU = print_train_time(start=train_time_start_CPU,\n",
    "                                  end=train_time_end_CPU,\n",
    "                                  device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb8d0a",
   "metadata": {},
   "source": [
    "#### Making predictions and get results of Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88626d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module,    # defining a custom function and defining parameters to be passed in\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary contraining the results of mdoel predicting on data_loader.\"\"\"\n",
    "    loss, acc = 0 , 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Batch loop\n",
    "        for image, label in data_loader:\n",
    "            # make prediction\n",
    "            label_pred = model(image)\n",
    "\n",
    "            # Accumulate the loss and accuracy val per batch\n",
    "            loss += loss_fn(label_pred, label)\n",
    "            acc += accuracy_fn(y_true=label,\n",
    "                               y_pred=label_pred.argmax(dim=1))\n",
    "            # END BATCH LOOP\n",
    "        # find average loss and acc per batch (len(data_loader) = # of items per batch)\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    return {\"model_name\": model.__class__.__name__, # only works if models are created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ecb240b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47143012285232544,\n",
       " 'model_acc': 83.57627795527156}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model results on test data\n",
    "\n",
    "model_0_results = eval_model(model = model_0,\n",
    "                             data_loader = test_dataloader,\n",
    "                             loss_fn = loss_fn,\n",
    "                             accuracy_fn = accuracy)\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55424d7d",
   "metadata": {},
   "source": [
    "## MODEL 1: NONlinear MNIST Model that's trained on the GPU\n",
    "this model also borrows some things from Model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2c6d311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed479049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "501fc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with non-linear & linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                      out_features=hidden_units,),\n",
    "            nn.ReLU(),  # NON LINEAR LAYER TYPE (relu)\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6dabcde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of model_1\n",
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape = 784,\n",
    "                              hidden_units=10,\n",
    "                              output_shape = len(class_names)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8853f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a loss and optimizer function.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
    "                            lr=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
