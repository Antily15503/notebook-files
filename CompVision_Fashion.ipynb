{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1510218e-d059-498d-9ac3-710a39e0a8da",
   "metadata": {},
   "source": [
    "## Machine Learning Notebook for Compter Vision using PyTorch\n",
    "`torchvision`: - base\n",
    "`torchvision.datasets` - get datasets and data loading functions for computer vision\n",
    "`torchvision.models` - get pretrained computer vision models that you can leverage for your own problems\n",
    "`torchvision.transofmrs` - functions for manipulating your vision data (images) to be suited for ML models\n",
    "`torch.utils.data.Dataset` - base dataset class for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981681d2-f9c2-41ab-9997-45fa8029cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import Torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46030c2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "FashionMNIST database: a database of pictures of clothes that's done in grayscale\n",
    "Download is included by PyTorch (in datasets list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9740fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where does the downloaded data go to?\n",
    "    train=True, # do we want the training dataset or testing dataset \n",
    "    download=True, #do we want to download\n",
    "    transform=torchvision.transforms.ToTensor(), # how do we want to transofrm the data? ()\n",
    "    target_transform=None # do we need to transform the labels (output)?\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # this is the testing dataset, so no train\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7648bd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes # command to see information of the different classes of our train_data\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c83bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = train_data.class_to_idx # same command as above but returns as a dictionary (class category paired with index)\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c8d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a DataLoader\n",
    "# A DataLoader turns a dataset (in this case images) into a Python iterable (batches / mini-batches of data sections)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set up a Batch Size HYPERPARAMATER\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef98de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 4, label size: <built-in method size of Tensor object at 0x000001E34DE06EE0>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOWING SAMPLES OF OUR MODEL'S DATA\n",
    "randomIndex = torch.randint(0, len(train_features_batch), size=[1]).item() # visualize one random sample from our batch (BATCH_SIZE)\n",
    "img, label = train_features_batch[randomIndex], train_labels_batch[randomIndex] # fetch the \"image\" and label from our batch at randomIndex\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\") # displays the image (in grayscale)\n",
    "plt.title(class_names[label]) # sets title to label string (since label is a tensor)\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7853ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a baseline computer vision model \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
